<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Infobesity  | Web scraping online insect survey</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.62.2" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.e08a958ae3e530145318b6373195c765.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="Web scraping online insect survey" />
<meta property="og:description" content="Using R to scrape data from HTML tables" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://infobesity.netlify.com/posts/ris-insect-survey/ris-insect-survey/" />
<meta property="article:published_time" content="2017-09-01T01:00:00+00:00" />
<meta property="article:modified_time" content="2017-09-01T01:00:00+00:00" />
<meta itemprop="name" content="Web scraping online insect survey">
<meta itemprop="description" content="Using R to scrape data from HTML tables">
<meta itemprop="datePublished" content="2017-09-01T01:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2017-09-01T01:00:00&#43;00:00" />
<meta itemprop="wordCount" content="3029">



<meta itemprop="keywords" content="Web Scraping,R," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Web scraping online insect survey"/>
<meta name="twitter:description" content="Using R to scrape data from HTML tables"/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://infobesity.netlify.com/posts/RIS-insect-survey/RIS-cover.jpeg');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://infobesity.netlify.com" class="f3 fw2 hover-white no-underline white-90 dib">
      Infobesity
    </a>
    <div class="flex-l items-center">
      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/" title="Homepage page">
              Homepage
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="Posts page">
              Posts
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About Me page">
              About Me
            </a>
          </li>
          
        </ul>
      
      









    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Web scraping online insect survey</h1>
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  <article class="flex-l flex-wrap justify-between mw8 center ph3 ph0-l">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">Web scraping online insect survey</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2017-09-01T01:00:00Z">September 1, 2017</time>
    </header>

    <main class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h2 id="introduction-to-rothamsteds-insect-survey">Introduction to Rothamsted's Insect Survey</h2>
<p>Rothamsted is the UK's leading agricultural research institutes. It is
also the world's oldest agricultural institute and still maintains the
longest runnning field experiment, which has been in operation since 1856. Rothamsted is also historically significant for statisticians with
Fisher and Anscombe working there post World War 2.</p>
<p>The Rothamsted Insect Survey (RIS) is a programme that maintains a
network of 16 traps around the UK emptied daily and publish aggregated
weekly data tables during Aphid season. The survey reports weekly count
data of the abundance of pest Aphids and can be found
<a href="https://insectsurvey.com/">here</a>.</p>
<p><em>Note, the pages holding the survey data have since been updated and this script will not scrape as it is.</em></p>
<p>This post is to document my process for web-scraping in R to extract data
from HTML tables over multiple pages and concatenating these into a
single time-series dataframe suitable for analysis.</p>
<h3 id="installed-packages">Installed Packages</h3>
<pre><code># Data Manipulation
library(dplyr)
library(stringr)
library(lubridate)
library(data.table)
library(reshape2)

# Web Scraping
library(rvest)
library(httr)

# Data Visualisation
library(ggplot2)
library(Amelia) # missmap function
</code></pre>
<h2 id="accessing-the-data">Accessing the Data</h2>
<p>Each Bulletin containing the HTML table of data is accessed through
several pages of links to individual URLs. The challenge is to extract
the tables at each url over each pages.</p>
<p>This creates a list of the parameters to the page URLs to loop through
and scrape from.</p>
<h3 id="pagination">Pagination</h3>
<p>This is a table of the pages that hold the URL links. Each of the
<code>PageURL</code>s will be concatenated onto the end of the base URL (<code>surveys</code>)
to access each page.</p>
<pre><code>surveys &lt;- read_html(&quot;http://resources.rothamsted.ac.uk/insect-survey/bulletins&quot;)

pagination &lt;-
    data.frame(
        PageName = &quot;Page 1&quot;,
        PageUrl = &quot;/&quot;
    )

pagination &lt;- rbind(
    pagination,
    data.frame(
        PageName = surveys %&gt;%
            html_nodes(&quot;li.pager-item a&quot;) %&gt;% # extracts anchor attributes from each 'next page' link
            html_attr(&quot;title&quot;) %&gt;% # extracts title-attribute from outputs
            gsub(&quot;Go to &quot;,&quot;&quot;, x = .) %&gt;% # cleaning up titles
            gsub(&quot;p&quot;, &quot;P&quot;, x = .),
        PageUrl = surveys %&gt;%
            html_nodes(&quot;li.pager-item a&quot;) %&gt;% # same as before
            html_attr(&quot;href&quot;) %&gt;% # extracts the href from attributes
            gsub(&quot;insect-survey/bulletins&quot;, &quot;&quot;, x = .))
    )

pagination

##   PageName  PageUrl
## 1   Page 1        /
## 2   Page 2 /?page=1
## 3   Page 3 /?page=2
## 4   Page 4 /?page=3
## 5   Page 5 /?page=4
## 6   Page 6 /?page=5
</code></pre>
<h3 id="extracting-elements-from-base-url">Extracting elements from Base URL</h3>
<p>With the pagination table, I can then loop through each page and extract
attributes from each link, using another loop, including the heading of
each Bulletin and the URL to the tables. This is stored in a dataframe
of all the links.</p>
<p>Not all the links contain weekly bulletins. The links that are
bulletins, containing the data, have a common naming pattern within
their title <code>&quot;Bulletin No: &quot;</code>. I used <code>grep</code> to only bring back those
instances that contain that pattern.</p>
<pre><code>all_bulletins &lt;- data.frame() # create empty dataframe for the bulletin list.

# Loop through each page in pagination dataframe, extract title and url of each link.

for (page in 1:nrow(pagination)) {
    page_url = paste(&quot;http://resources.rothamsted.ac.uk/insect-survey/bulletins&quot;, pagination$PageUrl[page], sep = &quot;&quot;)
    parsed_page = read_html(page_url)
    bulletins = data.frame(BulletinName = parsed_page %&gt;%
                               html_nodes(&quot;h4.title&quot;) %&gt;%
                               html_text(),
                           BulletinUrl = parsed_page %&gt;%
                               html_nodes(&quot;h4.title a&quot;) %&gt;%
                               html_attr(&quot;href&quot;)) %&gt;%
        dplyr::filter(
             grepl(pattern = &quot;Bulletin No: &quot;, BulletinName) # filters to only those with datasets
        )
    all_bulletins = rbind(all_bulletins, bulletins) # create dataframe concatenating all bulletins
    all_bulletins &lt;- droplevels(all_bulletins) # removes unused levels subsetted out
}

rm(page, page_url, parsed_page, surveys)

all_bulletins %&gt;% glimpse

## Observations: 122
## Variables: 2
## $ BulletinName &lt;fctr&gt; Bulletin No: 22. 21 August - 27 August 2017, Bul...
## $ BulletinUrl  &lt;fctr&gt; /insect-survey-bulletins/bulletin-no-22-21-augus...
</code></pre>
<h3 id="inspecting-the-dataframe">Inspecting the Dataframe</h3>
<p>The web scraping has extracted the name and the URL that links to the
data for each Bulletin over the 6 pages of results. As I will need a way
of knowing which data comes from each Bulletin after putting them all
together, I will extract information from the names here using Regular
Expressions.</p>
<pre><code># Regex to extract constituents from bulletin names

all_bulletins %&gt;%
    mutate(BulletinNo = str_extract(string = all_bulletins$BulletinName, &quot;Bulletin No: \\d+&quot;),
           year = str_extract(string = all_bulletins$BulletinName, &quot;\\d+$&quot;),
           date_range = str_replace(string = all_bulletins$BulletinName,
            pattern = &quot;Bulletin No: \\d+\\. &quot;,
            replacement = &quot;&quot;)
           ) -&gt; all_bulletins

tail(all_bulletins)

##                            BulletinName
## 117     Bulletin No: 7. 12 May - 18 May
## 118     Bulletin No: 6. 05 May - 11 May
## 119   Bulletin No: 5. 28 April - 04 May
## 120       Bulletin No: 2. 07 - 13 April
## 121       Bulletin No: 3. 14 - 20 April
## 122 Bulletin No: 1. 31 March - 06 April
##                                                  BulletinUrl
## 117     /insect-survey-bulletins/bulletin-no-7-12-may-18-may
## 118     /insect-survey-bulletins/bulletin-no-6-05-may-11-may
## 119   /insect-survey-bulletins/bulletin-no-5-28-april-04-may
## 120       /insect-survey-bulletins/bulletin-no-2-07-13-april
## 121       /insect-survey-bulletins/bulletin-no-3-14-20-april
## 122 /insect-survey-bulletins/bulletin-no-1-31-march-06-april
##         BulletinNo year          date_range
## 117 Bulletin No: 7 &lt;NA&gt;     12 May - 18 May
## 118 Bulletin No: 6 &lt;NA&gt;     05 May - 11 May
## 119 Bulletin No: 5 &lt;NA&gt;   28 April - 04 May
## 120 Bulletin No: 2 &lt;NA&gt;       07 - 13 April
## 121 Bulletin No: 3 &lt;NA&gt;       14 - 20 April
## 122 Bulletin No: 1 &lt;NA&gt; 31 March - 06 April
</code></pre>
<p>If we view this now, I have extracted and added as separate factors, the
Bulletin number, the year and the range of dates that the data covers.
This isn't the cleanest data, the earliest reports made in 2014 do not
have the year within the title, also the format of some of the date
ranges vary in how they were written.</p>
<p>Adding 2014 to the year manually:</p>
<pre><code>all_bulletins$year[is.na(all_bulletins$year)] &lt;- &quot;2014&quot;
</code></pre>
<h3 id="creating-an-aphid-collection-date">Creating an Aphid Collection Date</h3>
<p>This data is a time series and so I need to be able to extract a clean
date from each title. As mentioned before, the formats are not always
consistent:</p>
<pre><code>str_extract(string = all_bulletins$date_range,
            pattern = &quot;\\d+ \\w+ &quot;)

##   [1] &quot;21 August &quot;    &quot;14 August &quot;    &quot;07 August &quot;    &quot;31 July &quot;     
##   [5] &quot;24 July &quot;      &quot;17 July &quot;      &quot;10 July &quot;      &quot;02 July &quot;     
##   [9] &quot;26 June &quot;      &quot;19 June &quot;      &quot;12 June &quot;      &quot;05 June &quot;     
##  [13] &quot;29 May &quot;       &quot;22 May &quot;       &quot;15 May &quot;       &quot;08 May &quot;      
##  [17] &quot;01 May &quot;       &quot;24 April &quot;     &quot;17 April &quot;     &quot;10 April &quot;    
##  [21] &quot;03 April &quot;     &quot;27 March &quot;     &quot;07 November &quot;  &quot;06 November &quot;
##  [25] &quot;24 October &quot;   &quot;17 October &quot;   &quot;10 October &quot;   &quot;03 October &quot;  
##  [29] &quot;26 September &quot; &quot;19 September &quot; &quot;12 September &quot; &quot;05 September &quot;
##  [33] &quot;29 August &quot;    &quot;22 August &quot;    &quot;15 August &quot;    &quot;08 August &quot;   
##  [37] &quot;01 August &quot;    &quot;25 July &quot;      &quot;18 July &quot;      &quot;11 July &quot;     
##  [41] &quot;04 July &quot;      &quot;27 June &quot;      &quot;20 June &quot;      &quot;13 June &quot;     
##  [45] &quot;06 June &quot;      &quot;30 May &quot;       &quot;23 May &quot;       &quot;16 May &quot;      
##  [49] &quot;09 May &quot;       &quot;02 May &quot;       &quot;25 April &quot;     &quot;18 April &quot;    
##  [53] &quot;11 April &quot;     &quot;04 April &quot;     &quot;28 March &quot;     &quot;21 March &quot;    
##  [57] &quot;16 November &quot;  &quot;09 November &quot;  &quot;02 November &quot;  &quot;26 October &quot;  
##  [61] &quot;19 October &quot;   &quot;12 October &quot;   &quot;05 October &quot;   &quot;28 September &quot;
##  [65] &quot;21 September &quot; &quot;14 September &quot; &quot;07 September &quot; &quot;31 August &quot;   
##  [69] &quot;24 August &quot;    &quot;17 August &quot;    &quot;10 August &quot;    &quot;03 August &quot;   
##  [73] &quot;27 July &quot;      &quot;20 July &quot;      &quot;13 July &quot;      &quot;06 July &quot;     
##  [77] &quot;29 June &quot;      &quot;22 June &quot;      &quot;15 June &quot;      &quot;08 June &quot;     
##  [81] &quot;01 June &quot;      &quot;25 May &quot;       &quot;18 May &quot;       &quot;11 May &quot;      
##  [85] &quot;04 May &quot;       &quot;27 April &quot;     &quot;20 April &quot;     &quot;13 April &quot;    
##  [89] &quot;06 April &quot;     &quot;17 November &quot;  &quot;10 November &quot;  &quot;03 November &quot;
##  [93] &quot;27 October &quot;   &quot;20 October &quot;   &quot;13 October &quot;   &quot;06 October &quot;  
##  [97] &quot;29 September &quot; &quot;22 September &quot; &quot;15 September &quot; &quot;08 September &quot;
## [101] &quot;01 September &quot; &quot;25 August &quot;    &quot;18 August &quot;    &quot;11 August &quot;   
## [105] &quot;04 August &quot;    &quot;28 July &quot;      &quot;21 July &quot;      &quot;14 July &quot;     
## [109] &quot;07 July &quot;      &quot;30 June &quot;      &quot;23 June &quot;      &quot;16 June &quot;     
## [113] &quot;09 June &quot;      &quot;02 June &quot;      &quot;26 May &quot;       &quot;19 May &quot;      
## [117] &quot;12 May &quot;       &quot;05 May &quot;       &quot;28 April &quot;     NA             
## [121] NA              &quot;31 March &quot;
</code></pre>
<p>This shows that the naming format is not constant resulting in two
outputs with NA. As such I extract each element individually and
concatenate them to form a date that is the beginning of each data
collection exercise.</p>
<pre><code>paste(
# Match first numbers in each string
str_extract(string = all_bulletins$date_range,
            pattern = &quot;\\d+&quot;),
# Matches the month at beginning of collection
str_extract(string = all_bulletins$date_range,
            pattern = &quot;[:alpha:]+&quot;),
# already extracted the year
all_bulletins$year,
sep = &quot; &quot;
)

##   [1] &quot;21 August 2017&quot;    &quot;14 August 2017&quot;    &quot;07 August 2017&quot;   
##   [4] &quot;31 July 2017&quot;      &quot;24 July 2017&quot;      &quot;17 July 2017&quot;     
##   [7] &quot;10 July 2017&quot;      &quot;02 July 2017&quot;      &quot;26 June 2017&quot;     
##  [10] &quot;19 June 2017&quot;      &quot;12 June 2017&quot;      &quot;05 June 2017&quot;     
##  [13] &quot;29 May 2017&quot;       &quot;22 May 2017&quot;       &quot;15 May 2017&quot;      
##  [16] &quot;08 May 2017&quot;       &quot;01 May 2017&quot;       &quot;24 April 2017&quot;    
##  [19] &quot;17 April 2017&quot;     &quot;10 April 2017&quot;     &quot;03 April 2017&quot;    
##  [22] &quot;27 March 2017&quot;     &quot;07 November 2016&quot;  &quot;31 October 2016&quot;  
##  [25] &quot;24 October 2016&quot;   &quot;17 October 2016&quot;   &quot;10 October 2016&quot;  
##  [28] &quot;03 October 2016&quot;   &quot;26 September 2016&quot; &quot;19 September 2016&quot;
##  [31] &quot;12 September 2016&quot; &quot;05 September 2016&quot; &quot;29 August 2016&quot;   
##  [34] &quot;22 August 2016&quot;    &quot;15 August 2016&quot;    &quot;08 August 2016&quot;   
##  [37] &quot;01 August 2016&quot;    &quot;25 July 2016&quot;      &quot;18 July 2016&quot;     
##  [40] &quot;11 July 2016&quot;      &quot;04 July 2016&quot;      &quot;27 June 2016&quot;     
##  [43] &quot;20 June 2016&quot;      &quot;13 June 2016&quot;      &quot;06 June 2016&quot;     
##  [46] &quot;30 May 2016&quot;       &quot;23 May 2016&quot;       &quot;16 May 2016&quot;      
##  [49] &quot;09 May 2016&quot;       &quot;02 May 2016&quot;       &quot;25 April 2016&quot;    
##  [52] &quot;18 April 2016&quot;     &quot;11 April 2016&quot;     &quot;04 April 2016&quot;    
##  [55] &quot;28 March 2016&quot;     &quot;21 March 2016&quot;     &quot;16 November 2015&quot;
##  [58] &quot;09 November 2015&quot;  &quot;02 November 2015&quot;  &quot;26 October 2015&quot;  
##  [61] &quot;19 October 2015&quot;   &quot;12 October 2015&quot;   &quot;05 October 2015&quot;  
##  [64] &quot;28 September 2015&quot; &quot;21 September 2015&quot; &quot;14 September 2015&quot;
##  [67] &quot;07 September 2015&quot; &quot;31 August 2015&quot;    &quot;24 August 2015&quot;   
##  [70] &quot;17 August 2015&quot;    &quot;10 August 2015&quot;    &quot;03 August 2015&quot;   
##  [73] &quot;27 July 2015&quot;      &quot;20 July 2015&quot;      &quot;13 July 2015&quot;     
##  [76] &quot;06 July 2015&quot;      &quot;29 June 2015&quot;      &quot;22 June 2015&quot;     
##  [79] &quot;15 June 2015&quot;      &quot;08 June 2015&quot;      &quot;01 June 2015&quot;     
##  [82] &quot;25 May 2015&quot;       &quot;18 May 2015&quot;       &quot;11 May 2015&quot;      
##  [85] &quot;04 May 2015&quot;       &quot;27 April 2015&quot;     &quot;20 April 2015&quot;    
##  [88] &quot;13 April 2015&quot;     &quot;06 April 2015&quot;     &quot;17 November 2014&quot;
##  [91] &quot;10 November 2014&quot;  &quot;03 November 2014&quot;  &quot;27 October 2014&quot;  
##  [94] &quot;20 October 2014&quot;   &quot;13 October 2014&quot;   &quot;06 October 2014&quot;  
##  [97] &quot;29 September 2014&quot; &quot;22 September 2014&quot; &quot;15 September 2014&quot;
## [100] &quot;08 September 2014&quot; &quot;01 September 2014&quot; &quot;25 August 2014&quot;   
## [103] &quot;18 August 2014&quot;    &quot;11 August 2014&quot;    &quot;04 August 2014&quot;   
## [106] &quot;28 July 2014&quot;      &quot;21 July 2014&quot;      &quot;14 July 2014&quot;     
## [109] &quot;07 July 2014&quot;      &quot;30 June 2014&quot;      &quot;23 June 2014&quot;     
## [112] &quot;16 June 2014&quot;      &quot;09 June 2014&quot;      &quot;02 June 2014&quot;     
## [115] &quot;26 May 2014&quot;       &quot;19 May 2014&quot;       &quot;12 May 2014&quot;      
## [118] &quot;05 May 2014&quot;       &quot;28 April 2014&quot;     &quot;07 April 2014&quot;    
## [121] &quot;14 April 2014&quot;     &quot;31 March 2014&quot;
</code></pre>
<p>Adding this into the <code>all_bulletins</code> dataframe as a new factor, and give
this a date fomat.</p>
<pre><code>all_bulletins %&gt;%
    mutate(week_start =
               paste(
str_extract(string = all_bulletins$date_range,
            pattern = &quot;\\d+&quot;),
# Matches the Week beginning month
str_extract(string = all_bulletins$date_range,
            pattern = &quot;[:alpha:]+&quot;),
# already extracted the year
all_bulletins$year,
sep = &quot; &quot;)
) -&gt; all_bulletins

all_bulletins$week_start &lt;- dmy(all_bulletins$week_start)

str(all_bulletins)

## 'data.frame':    122 obs. of  6 variables:
##  $ BulletinName: Factor w/ 122 levels &quot;Bulletin No: 1. 27 March - 02 April 2017&quot;,..: 15 14 13 11 10 9 8 7 6 5 ...
##  $ BulletinUrl : Factor w/ 122 levels &quot;/insect-survey-bulletins/bulletin-no-1-27-march-02-april-2017&quot;,..: 15 14 13 11 10 9 8 7 6 5 ...
##  $ BulletinNo  : chr  &quot;Bulletin No: 22&quot; &quot;Bulletin No: 21&quot; &quot;Bulletin No: 20&quot; &quot;Bulletin No: 19&quot; ...
##  $ year        : chr  &quot;2017&quot; &quot;2017&quot; &quot;2017&quot; &quot;2017&quot; ...
##  $ date_range  : chr  &quot;21 August - 27 August 2017&quot; &quot;14 August - 20 August 2017&quot; &quot;07 August - 13 August 2017&quot; &quot;31 July - 06 August 2017&quot; ...
##  $ week_start  : Date, format: &quot;2017-08-21&quot; &quot;2017-08-14&quot; ...
</code></pre>
<h2 id="extracting-the-data">Extracting the Data</h2>
<p>Everything up until now has been preparing for extracting the data. To
concatenate all the data together for analysis, a list is created and
the data from each table is scraped and placed into a dataframe and
placed within the list. Each dataframe in this contains the table
extracted from the site location.</p>
<p>This list is then concatenated together using <code>dplyr::bind_rows()</code>. The
base function <code>rbind()</code> requires columns within each table to be the
same. In this case, over such a timescale the locations of the insect
traps may not stay constant over the years. The function <code>bind_rows()</code>
will retain all columns and place &lsquo;NA&rsquo; in the spaces.</p>
<pre><code>data.list = list()

for(index in (1:nrow(all_bulletins))) {
    tryCatch({
        paste(&quot;http://resources.rothamsted.ac.uk&quot;,all_bulletins$BulletinUrl[index], sep = &quot;&quot;) %&gt;%
        read_html %&gt;%
        html_table(., header = TRUE) %&gt;%
        do.call(cbind, .) -&gt; dat

    dat[is.na(dat)] &lt;- 0 # assigns all NAs to 0

    dat %&gt;%
        mutate(year = all_bulletins$year[index],
               bulletin.number = all_bulletins$BulletinNo[index],
               week_start = all_bulletins$week_start[index]) -&gt; dat

    dat$index &lt;- index

    data.list[[index]] &lt;- dat
    }, error=function(e){})
}

## Warning: closing unused connection 5 (http://resources.rothamsted.ac.uk/
## insect-survey-bulletins/bulletin-no-34-07-november-13-november-2016)
</code></pre>
<p>This outputs a warning about closing an unused connection with the
number and the URL. This informs us that bulletin number 34 from the 7th
November to 13th November 2016 could not be accessed. This was the
purpose of the <code>tryCatch</code>, without which the loop would stop executing
the script after receiving the error.</p>
<pre><code>all.data &lt;- do.call(bind_rows, data.list)
</code></pre>
<p>Within this, I am assuming that where cells within the table are blank,
no aphids of that specied were found and have been replaced with a <code>0</code>.
This is to differentiate when a location is not available / functioning
which will contain <code>NA</code>.</p>
<p>Renaming some columns</p>
<pre><code>data.table::setnames(all.data,
                     old = c('Var.1','year', 'bulletin.number', 'index'),
                     new = c(&quot;Aphid_sp&quot;, &quot;Year&quot;, &quot;Bulletin_Number&quot;, &quot;Index&quot;)
                     )
</code></pre>
<p>In the data there are observations that lists the number of part catches
and number of days of catching. Removing these.</p>
<pre><code>all.data %&gt;%
    filter(Aphid_sp != &quot;Days&quot;) %&gt;%
    filter(Aphid_sp != &quot;Part Catches&quot;) -&gt; all.data
</code></pre>
<h3 id="inspecting-the-final-dataset">Inspecting the Final Dataset</h3>
<pre><code>summary(all.data)

##    Aphid_sp               El                 D                 G          
##  Length:2541        Min.   :   0.000   Min.   :  0.000   Min.   :   0.00  
##  Class :character   1st Qu.:   0.000   1st Qu.:  0.000   1st Qu.:   0.00  
##  Mode  :character   Median :   0.000   Median :  0.000   Median :   0.00  
##                     Mean   :   6.504   Mean   :  7.537   Mean   :  20.74  
##                     3rd Qu.:   0.000   3rd Qu.:  1.000   3rd Qu.:   2.00  
##                     Max.   :1048.000   Max.   :996.000   Max.   :6156.00  
##                                                                           
##        Ay                N                 Y                P           
##  Min.   :  0.000   Min.   :   0.00   Min.   :  0.00   Min.   :    0.00  
##  1st Qu.:  0.000   1st Qu.:   0.00   1st Qu.:  0.00   1st Qu.:    0.00  
##  Median :  0.000   Median :   0.00   Median :  0.00   Median :    0.00  
##  Mean   :  3.731   Mean   :  10.88   Mean   : 11.75   Mean   :   74.06  
##  3rd Qu.:  0.000   3rd Qu.:   1.00   3rd Qu.:  4.00   3rd Qu.:    3.00  
##  Max.   :667.000   Max.   :1895.00   Max.   :857.00   Max.   :18287.00  
##                                      NA's   :693                        
##        K                 BB                We                H          
##  Min.   :   0.00   Min.   :   0.00   Min.   :   0.00   Min.   :   0.00  
##  1st Qu.:   0.00   1st Qu.:   0.00   1st Qu.:   0.00   1st Qu.:   0.00  
##  Median :   0.00   Median :   0.00   Median :   0.00   Median :   0.00  
##  Mean   :  13.18   Mean   :  14.23   Mean   :  12.29   Mean   :  12.58  
##  3rd Qu.:   4.00   3rd Qu.:   3.00   3rd Qu.:   3.00   3rd Qu.:   3.00  
##  Max.   :1847.00   Max.   :1942.00   Max.   :1368.00   Max.   :3053.00  
##                                                                         
##        RT                 Wr                SP                W          
##  Min.   :   0.000   Min.   :   0.00   Min.   :  0.000   Min.   :  0.000  
##  1st Qu.:   0.000   1st Qu.:   0.00   1st Qu.:  0.000   1st Qu.:  0.000  
##  Median :   0.000   Median :   0.00   Median :  0.000   Median :  0.000  
##  Mean   :   9.289   Mean   :  12.45   Mean   :  3.553   Mean   :  6.149  
##  3rd Qu.:   3.000   3rd Qu.:   3.00   3rd Qu.:  1.000   3rd Qu.:  2.000  
##  Max.   :1627.000   Max.   :1381.00   Max.   :639.000   Max.   :510.000  
##                                                                          
##        SX             Year           Bulletin_Number   
##  Min.   :  0.00   Length:2541        Length:2541       
##  1st Qu.:  0.00   Class :character   Class :character  
##  Median :  0.00   Mode  :character   Mode  :character  
##  Mean   :  7.58                                        
##  3rd Qu.:  3.00                                        
##  Max.   :664.00                                        
##                                                        
##    week_start             Index              AB      
##  Min.   :2014-03-31   Min.   :  1.00   Min.   :0     
##  1st Qu.:2014-11-03   1st Qu.: 32.00   1st Qu.:0     
##  Median :2015-10-12   Median : 62.00   Median :0     
##  Mean   :2015-11-24   Mean   : 61.82   Mean   :0     
##  3rd Qu.:2016-09-05   3rd Qu.: 92.00   3rd Qu.:0     
##  Max.   :2017-08-21   Max.   :122.00   Max.   :0     
##                                        NA's   :2247

Amelia::missmap(all.data,
                col = c(&quot;steelblue&quot;, &quot;lightgrey&quot;),
                y.labels = c(2500, 2250, 2000, 1750, 1500, 1250, 1000, 750, 500, 250, 1),
                y.at = c(1, 250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500)+20,
                main = &quot;Missing data Map&quot;)
</code></pre>
<p><img src="/posts/RIS-insect-survey/images/missing_map.png" alt="image"></p>
<p>This indicates that the collection traps at <code>AB</code> were not available for
chunks of time at the beginning of the period. Also the collection at
<code>Y</code> location was discontinued.</p>
<p>Plotting Aphid species counts for Rothamsted Tower.</p>
<pre><code>all.data %&gt;%
    ggplot(aes(x = week_start, y = RT)) +
    geom_point() +
    theme_bw() +
    labs(
        title = &quot;Aphid Species Captured at Rothamsted Tower&quot;,
        x = &quot;Collection Date&quot;,
        y = &quot;Aphid Counts&quot;)
</code></pre>
<p><img src="/posts/RIS-insect-survey/images/results.png" alt="image"></p>
<p>From this point the data can be manipulated and cleaned as required for
analysis.</p>
<h2 id="session-info">Session Info</h2>
<pre><code>## R version 3.3.3 (2017-03-06)
## Platform: x86_64-apple-darwin13.4.0 (64-bit)
## Running under: macOS Sierra 10.12.6
##
## locale:
## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
##
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
##
## other attached packages:
##  [1] Amelia_1.7.4      Rcpp_0.12.12      ggplot2_2.2.1    
##  [4] httr_1.2.1        rvest_0.3.2       xml2_1.1.1       
##  [7] reshape2_1.4.2    data.table_1.10.0 lubridate_1.6.0  
## [10] stringr_1.2.0     dplyr_0.5.0      
##
## loaded via a namespace (and not attached):
##  [1] knitr_1.16       magrittr_1.5     munsell_0.4.3    colorspace_1.3-2
##  [5] R6_2.2.0         plyr_1.8.4       tools_3.3.3      grid_3.3.3      
##  [9] gtable_0.2.0     DBI_0.5-1        selectr_0.3-1    htmltools_0.3.6
## [13] lazyeval_0.2.0   yaml_2.1.14      assertthat_0.1   rprojroot_1.2   
## [17] digest_0.6.12    tibble_1.2       curl_2.3         evaluate_0.10.1
## [21] rmarkdown_1.6    labeling_0.3     stringi_1.1.5    scales_0.4.1    
## [25] backports_1.0.5  XML_3.98-1.5     foreign_0.8-67</code></pre>
<ul class="pa0">
  
   <li class="list">
     <a href="/tags/web-scraping" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Web Scraping</a>
   </li>
  
   <li class="list">
     <a href="/tags/r" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">R</a>
   </li>
  
</ul>
<div class="mt6">
        
      </div>
    </main>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://infobesity.netlify.com" >
    &copy; 2020 Infobesity
  </a>
    <div>








</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
